__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import streamlit as st
import json
import re
import os
import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer, CrossEncoder
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
import pandas as pd
import time
import torch
from datetime import datetime
import random
import numpy as np
from typing import List, Dict, Any
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="ì„ë² ë”© í†µí•© ë²¤ì¹˜ë§ˆí¬", layout="wide", page_icon="ğŸš€")
st.title("ğŸš€ ì„ë² ë”© ëª¨ë¸ í†µí•© ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ")

# GPU ì²´í¬
device = "cuda" if torch.cuda.is_available() else "cpu"
st.info(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device} ğŸ®" if device == "cuda" else f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device} ğŸ’»")

# --- ì„¤ì • ---
CHROMA_DB_PATH = r"C:\auto_excel\chromadb"

AVAILABLE_MODELS = {
    "jhgan/ko-sroberta-multitask": "naver_blogs_sroberta_768d",
    "BM-K/KoSimCSE-roberta": "naver_blogs_kosimcse_768d", 
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "naver_blogs_minilm_384d",
    "sentence-transformers/distiluse-base-multilingual-cased-v1": "naver_blogs_distiluse_512d",
    "intfloat/multilingual-e5-base": "naver_blogs_e5base_768d",
    "BAAI/bge-large-en-v1.5": "naver_blogs_bge_large_en_1024d",
    "snunlp/KR-SBERT-V40K-klueNLI-augSTS": "naver_blogs_krsbert_768d",
    "intfloat/multilingual-e5-large": "naver_blogs_e5_large_1024d"
}

# --- íƒ­ ìƒì„± ---
tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“ ë°ì´í„° ì¤€ë¹„", "ğŸ¤– LLM ì •í™•í•œ ë²¤ì¹˜ë§ˆí¬", "ğŸ”§ ì»¬ë ‰ì…˜ êµ¬ì¶•", "ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰", "ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„"])

# --- í—¬í¼ í•¨ìˆ˜ë“¤ ---
@st.cache_resource
def get_chroma_client():
    return chromadb.PersistentClient(path=CHROMA_DB_PATH)

@st.cache_resource
def load_embedding_model(model_name):
    with st.spinner(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}..."):
        return SentenceTransformer(model_name, device=device)

@st.cache_resource  
def load_reranker_model(model_name="BAAI/bge-reranker-v2-m3"):
    with st.spinner(f"Re-ranker ëª¨ë¸ ë¡œë”© ì¤‘: {model_name.split('/')[-1]}..."):
        return CrossEncoder(model_name, device=device)

def build_vector_database(json_data, model_name, collection_name, chunk_size=500, chunk_overlap=50):
    """JSON ë°ì´í„°ë¡œë¶€í„° ChromaDB ì»¬ë ‰ì…˜ì„ êµ¬ì¶•"""
    items = json_data.get('items', json_data)
    
    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)
    chroma_client = get_chroma_client()
    
    try:
        chroma_client.delete_collection(name=collection_name)
        st.info(f"ê¸°ì¡´ '{collection_name}' ì»¬ë ‰ì…˜ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
    except Exception:
        pass
    
    collection = chroma_client.create_collection(
        name=collection_name,
        embedding_function=embedding_func,
        metadata={"description": f"{model_name} ì„ë² ë”© ì‚¬ìš© (ì²­í‚¹ ì ìš©)"}
    )
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
    )
    
    ids, documents, metadatas = [], [], []
    progress_bar = st.progress(0, text="ì²­í‚¹ ì§„í–‰ ì¤‘...")
    
    for i, item in enumerate(items):
        original_id = str(item.get('id', f"item_{i}"))
        title = re.sub('<.*?>', '', str(item.get('title', '')))
        content = re.sub('<.*?>', '', str(item.get('description', '')))
        full_content = title + "\n\n" + content
        
        chunks = text_splitter.split_text(full_content)
        
        for j, chunk in enumerate(chunks):
            chunk_id = f"{original_id}_chunk_{j}"
            ids.append(chunk_id)
            documents.append(chunk)
            metadatas.append({
                "original_id": original_id,
                "chunk_number": j,
                "title": title,
                "url": item.get('link', ''),
                "date": item.get('postdate', ''),
            })
        
        progress_bar.progress((i + 1) / len(items), text=f"ì²­í‚¹ ì§„í–‰ ì¤‘... ({i+1}/{len(items)})")
    
    if ids:
        collection.add(ids=ids, documents=documents, metadatas=metadatas)
        st.success(f"âœ… ì´ {len(items)}ê°œ ë¬¸ì„œì—ì„œ {len(ids)}ê°œì˜ ì¡°ê°ì„ ìƒì„±í•˜ì—¬ '{collection_name}'ì— ì €ì¥ì™„ë£Œ!")
    
    return collection

def calculate_detailed_metrics(ranked_docs, relevant_docs, k=10):
    """ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚° (MRR, NDCG ë“±)"""
    # MRR ê³„ì‚°
    mrr = 0.0
    for i, doc_id in enumerate(ranked_docs):
        if any(doc_id.startswith(rel + "_") for rel in relevant_docs):
            mrr = 1.0 / (i + 1)
            break
    
    # NDCG ê³„ì‚°
    dcg = 0.0
    for i, doc_id in enumerate(ranked_docs[:k]):
        if any(doc_id.startswith(rel + "_") for rel in relevant_docs):
            dcg += 1.0 / np.log2(i + 2)
    
    ideal_dcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant_docs), k)))
    ndcg = dcg / ideal_dcg if ideal_dcg > 0 else 0.0
    
    # Precision@K
    precision_at_k = sum(
        1 for doc_id in ranked_docs[:k] 
        if any(doc_id.startswith(rel + "_") for rel in relevant_docs)
    ) / k
    
    return {"mrr": mrr, "ndcg": ndcg, "precision_at_k": precision_at_k}

# --- ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜ë“¤ ---
def create_model_comparison_chart(benchmark_results):
    """ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ - ì˜¤ë¥˜ ì²˜ë¦¬ ê°œì„ """
    df = pd.DataFrame(benchmark_results)
    
    if df.empty:
        st.error("âŒ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # Hit Rate ì»¬ëŸ¼ ì°¾ê¸°
    hit_rate_cols = [col for col in df.columns if "Hit Rate" in col]
    if not hit_rate_cols:
        st.error("âŒ Hit Rate ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    hit_rate_col = hit_rate_cols[0]
    
    # ì˜¤ë¥˜ ë¬¸ìì—´ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    def safe_convert_to_float(value):
        try:
            if isinstance(value, str):
                if 'ì˜¤ë¥˜' in value or 'Error' in value:
                    return 0.0
                clean_value = value.replace('%', '').replace('+', '').strip()
                return float(clean_value)
            return float(value)
        except (ValueError, TypeError):
            return 0.0
    
    df['hit_rate_numeric'] = df[hit_rate_col].apply(safe_convert_to_float)
    df['model_short'] = df['Model'].apply(lambda x: x.split('/')[-1] if isinstance(x, str) else str(x))
    
    # ìœ íš¨í•œ ê²°ê³¼ë§Œ í•„í„°ë§
    valid_df = df[df['hit_rate_numeric'] > 0]
    
    if len(valid_df) == 0:
        st.error("âš ï¸ ìœ íš¨í•œ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ëª¨ë¸ì—ì„œ ì˜¤ë¥˜ ë°œìƒ")
        return None
    
    fig = px.bar(
        valid_df, 
        x='model_short', 
        y='hit_rate_numeric',
        color='hit_rate_numeric',
        color_continuous_scale='Viridis',
        title='ğŸ“Š ëª¨ë¸ë³„ Hit Rate ì„±ëŠ¥ ë¹„êµ',
        labels={'hit_rate_numeric': 'Hit Rate (%)', 'model_short': 'ëª¨ë¸ëª…'}
    )
    
    # ì˜¬ë°”ë¥¸ Plotly ë¬¸ë²•
    fig.update_layout(
        xaxis_tickangle=-45,
        height=500,
        showlegend=False
    )
    
    return fig

def create_reranker_comparison_chart(results_with_without_reranker):
    """Re-ranker ì‚¬ìš© ì „í›„ ë¹„êµ ì°¨íŠ¸"""
    if not results_with_without_reranker:
        return None
        
    df = pd.DataFrame(results_with_without_reranker)
    
    fig = go.Figure()
    
    fig.add_trace(go.Bar(
        name='Re-ranker ë¯¸ì‚¬ìš©',
        x=df['Model'],
        y=df['Without_Reranker'],
        marker_color='lightcoral'
    ))
    
    fig.add_trace(go.Bar(
        name='Re-ranker ì‚¬ìš©',
        x=df['Model'],
        y=df['With_Reranker'],
        marker_color='lightblue'
    ))
    
    fig.update_layout(
        title='ğŸ”„ Re-ranker ì‚¬ìš© ì „í›„ ì„±ëŠ¥ ë¹„êµ',
        xaxis_title='ëª¨ë¸ëª…',
        yaxis_title='Hit Rate (%)',
        barmode='group',
        height=500,
        xaxis_tickangle=-45
    )
    
    return fig

def create_k_value_analysis_chart(k_analysis_results):
    """Kê°’ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ë¶„ì„"""
    fig = go.Figure()
    
    for model, results in k_analysis_results.items():
        fig.add_trace(go.Scatter(
            x=results['k_values'],
            y=results['hit_rates'],
            mode='lines+markers',
            name=model.split('/')[-1],
            line=dict(width=3),
            marker=dict(size=8)
        ))
    
    fig.update_layout(
        title='ğŸ“ˆ Kê°’ì— ë”°ë¥¸ Hit Rate ë³€í™”',
        xaxis_title='K ê°’',
        yaxis_title='Hit Rate (%)',
        height=500,
        hovermode='x unified'
    )
    
    return fig

def create_question_type_analysis_chart(question_type_results):
    """ì§ˆë¬¸ ìœ í˜•ë³„ ì„±ëŠ¥ ë¶„ì„"""
    fig = px.box(
        question_type_results,
        x='question_type',
        y='hit_rate',
        color='question_type',
        title='ğŸ¯ ì§ˆë¬¸ ìœ í˜•ë³„ ì„±ëŠ¥ ë¶„í¬',
        labels={'question_type': 'ì§ˆë¬¸ ìœ í˜•', 'hit_rate': 'Hit Rate (%)'}
    )
    
    fig.update_layout(height=500)
    return fig

def create_detailed_metrics_radar_chart(detailed_metrics):
    """ìƒì„¸ ë©”íŠ¸ë¦­ ë ˆì´ë” ì°¨íŠ¸"""
    if not detailed_metrics:
        return None
        
    fig = go.Figure()
    
    metrics = ['Hit Rate', 'MRR', 'NDCG', 'Precision@K']
    
    for model, values in detailed_metrics.items():
        model_short = model.split('/')[-1]
        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=metrics,
            fill='toself',
            name=model_short
        ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 1]
            )),
        showlegend=True,
        title='ğŸ•¸ï¸ ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸',
        height=500
    )
    
    return fig

# ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë²¤ì¹˜ë§ˆí¬ í•¨ìˆ˜
def debug_benchmark_performance(model_name, collection_name, benchmark_data, sample_size=5):
    """ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ ë””ë²„ê¹…"""
    try:
        retriever_model = load_embedding_model(model_name)
        chroma_client = get_chroma_client()
        collection = chroma_client.get_collection(name=collection_name)
        
        st.write(f"ğŸ” **{model_name.split('/')[-1]} ë””ë²„ê¹…**")
        st.write(f"- ì»¬ë ‰ì…˜: {collection_name}")
        st.write(f"- ì´ ë¬¸ì„œ ìˆ˜: {collection.count()}")
        
        # ìƒ˜í”Œ ì§ˆë¬¸ë“¤ë¡œ í…ŒìŠ¤íŠ¸
        sample_items = benchmark_data[:sample_size]
        
        for i, item in enumerate(sample_items):
            st.write(f"\n**ì§ˆë¬¸ {i+1}**: {item['query']}")
            st.write(f"**ì •ë‹µ ë¬¸ì„œ**: {item['relevant_docs']}")
            
            # ê²€ìƒ‰ ì‹¤í–‰
            query_embedding = retriever_model.encode(item['query']).tolist()
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=10,
                include=["metadatas"]
            )
            
            retrieved_ids = results['ids'][0]
            st.write(f"**ê²€ìƒ‰ëœ ìƒìœ„ 10ê°œ**: {retrieved_ids}")
            
            # ì²­í‚¹ëœ ID ë§¤ì¹­ í™•ì¸
            relevant_docs = set(item['relevant_docs'])
            matches = []
            
            for r_id in retrieved_ids:
                for ans_id in relevant_docs:
                    if r_id.startswith(ans_id + "_"):
                        matches.append(r_id)
                        break
            
            if matches:
                st.success(f"âœ… **ë§¤ì¹­ ì„±ê³µ**: {matches}")
            else:
                st.error(f"âŒ **ë§¤ì¹­ ì‹¤íŒ¨**: ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨")
                
                # ì‹¤ì œ ì»¬ë ‰ì…˜ì— ìˆëŠ” IDë“¤ ìƒ˜í”Œ í™•ì¸
                all_results = collection.query(
                    query_embeddings=[query_embedding],
                    n_results=50
                )
                all_ids = all_results['ids'][0]
                
                # ì •ë‹µ IDë¡œ ì‹œì‘í•˜ëŠ” ì²­í¬ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
                answer_chunks = []
                for ans_id in relevant_docs:
                    matching_chunks = [chunk_id for chunk_id in all_ids if chunk_id.startswith(ans_id + "_")]
                    answer_chunks.extend(matching_chunks)
                
                if answer_chunks:
                    st.warning(f"âš ï¸ **ì •ë‹µ ì²­í¬ë“¤ì´ ì¡´ì¬í•˜ì§€ë§Œ ìƒìœ„ 10ê°œì— ì—†ìŒ**: {answer_chunks[:5]}")
                else:
                    st.error(f"ğŸ’¥ **ì •ë‹µ ì²­í¬ë“¤ì´ ì»¬ë ‰ì…˜ì— ì•„ì˜ˆ ì—†ìŒ**. ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì™€ ì»¬ë ‰ì…˜ ë¶ˆì¼ì¹˜!")
            
            st.write("---")
            
    except Exception as e:
        st.error(f"ë””ë²„ê¹… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def check_collection_health():
    """ì»¬ë ‰ì…˜ ìƒíƒœ ë° ë°ì´í„° ì¼ê´€ì„± í™•ì¸"""
    try:
        chroma_client = get_chroma_client()
        collections = chroma_client.list_collections()
        
        st.subheader("ğŸ¥ ì»¬ë ‰ì…˜ ê±´ê°• ìƒíƒœ ì²´í¬")
        
        health_data = []
        for collection_info in collections:
            collection = chroma_client.get_collection(collection_info.name)
            count = collection.count()
            
            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
            sample = collection.get(limit=5, include=['ids', 'metadatas'])
            sample_ids = sample.get('ids', [])
            
            # ID íŒ¨í„´ í™•ì¸ (ì²­í‚¹ë˜ì—ˆëŠ”ì§€)
            chunked_ids = [id for id in sample_ids if '_chunk_' in id]
            chunked_ratio = len(chunked_ids) / len(sample_ids) * 100 if sample_ids else 0
            
            health_data.append({
                'Collection': collection_info.name,
                'Document Count': count,
                'Sample IDs': ', '.join(sample_ids[:3]),
                'Chunked Ratio': f"{chunked_ratio:.1f}%",
                'Status': 'âœ… ì •ìƒ' if count > 0 and chunked_ratio > 0 else 'âš ï¸ ë¬¸ì œ'
            })
        
        health_df = pd.DataFrame(health_data)
        st.dataframe(health_df, use_container_width=True)
        
        return health_df
        
    except Exception as e:
        st.error(f"ì»¬ë ‰ì…˜ ìƒíƒœ ì²´í¬ ì‹¤íŒ¨: {e}")
        return None

# --- TAB 1: ë°ì´í„° ì¤€ë¹„ ---
with tab1:
    st.header("ğŸ“ ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ")
    
    data_source = st.radio("ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”:", 
                          ["ë¡œì»¬ íŒŒì¼ ê²½ë¡œ", "íŒŒì¼ ì—…ë¡œë“œ"])
    
    json_data = None
    
    if data_source == "ë¡œì»¬ íŒŒì¼ ê²½ë¡œ":
        file_path = st.text_input("JSON íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”:", 
                                 value=r"C:\auto_excel\naver_blog_ëª¨ê¸°ìš©í’ˆ_20250617_131814.json")
        
        if st.button("íŒŒì¼ ë¡œë“œ") and file_path:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        json_data = json.load(f)
                    st.session_state['json_data'] = json_data
                    items = json_data.get('items', json_data)
                    st.success(f"âœ… íŒŒì¼ ë¡œë“œ ì„±ê³µ! ì´ {len(items)}ê°œ ë¬¸ì„œ")
                    
                    if items:
                        st.write("**ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:**")
                        preview_item = items[0]
                        st.json({k: str(v)[:100] + "..." if len(str(v)) > 100 else v 
                                for k, v in preview_item.items()})
                        
                except Exception as e:
                    st.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                st.error("íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    else:
        uploaded_file = st.file_uploader("JSON íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”:", type=['json'])
        
        if uploaded_file:
            try:
                json_data = json.load(uploaded_file)
                st.session_state['json_data'] = json_data
                items = json_data.get('items', json_data)
                st.success(f"âœ… íŒŒì¼ ì—…ë¡œë“œ ì„±ê³µ! ì´ {len(items)}ê°œ ë¬¸ì„œ")
                
                if items:
                    st.write("**ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:**")
                    preview_item = items[0]
                    st.json({k: str(v)[:100] + "..." if len(str(v)) > 100 else v 
                            for k, v in preview_item.items()})
                    
            except Exception as e:
                st.error(f"íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {e}")

# --- TAB 2: LLM ê¸°ë°˜ ì •í™•í•œ ë²¤ì¹˜ë§ˆí¬ ìƒì„± ---
with tab2:
    st.header("ğŸ¤– LLM ê¸°ë°˜ ì •í™•í•œ ë²¤ì¹˜ë§ˆí¬ ìƒì„±")
    st.success("ğŸ¯ **ì¶”ì²œ ë°©ì‹**: ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ì„ ì½ê³  ì •í™•í•œ ì§ˆë¬¸-ì •ë‹µ ìŒì„ ìƒì„±í•©ë‹ˆë‹¤")
    
    # OpenAI API í‚¤ ì…ë ¥
    col1, col2 = st.columns([2, 1])
    
    with col1:
        api_key = st.text_input(
            "OpenAI API í‚¤", 
            type="password",
            help="gpt-4o-mini ëª¨ë¸ ì‚¬ìš© (ë¹„ìš©: ~$0.01/100ì§ˆë¬¸)"
        )
    
    with col2:
        if st.button("ğŸ” API í‚¤ í…ŒìŠ¤íŠ¸"):
            if api_key:
                try:
                    from openai import OpenAI
                    test_client = OpenAI(api_key=api_key)
                    test_client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": "í…ŒìŠ¤íŠ¸"}],
                        max_tokens=5
                    )
                    st.success("âœ… API í‚¤ ìœ íš¨")
                except Exception as e:
                    st.error(f"âŒ API í‚¤ ì˜¤ë¥˜: {str(e)[:50]}")
            else:
                st.warning("API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    if not api_key or "sk-" not in api_key:
        st.warning("â¬†ï¸ ë¨¼ì € OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
        
        with st.expander("ğŸ’¡ OpenAI API í‚¤ ë°œê¸‰ ë°©ë²•"):
            st.markdown("""
            1. **https://platform.openai.com** ì ‘ì†
            2. **API keys** ë©”ë‰´ í´ë¦­
            3. **Create new secret key** í´ë¦­
            4. ìƒì„±ëœ í‚¤ë¥¼ ë³µì‚¬í•´ì„œ ìœ„ì— ì…ë ¥
            
            ğŸ’° **ë¹„ìš©**: gpt-4o-mini ëª¨ë¸ ì‚¬ìš© ì‹œ 100ê°œ ì§ˆë¬¸ë‹¹ ì•½ $0.01
            """)
    
    else:
        # LLM ë²¤ì¹˜ë§ˆí¬ ìƒì„± ì¸í„°í˜ì´ìŠ¤
        st.subheader("âš™ï¸ ìƒì„± ì„¤ì •")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ í™•ì¸
            try:
                chroma_client = get_chroma_client()
                collections = chroma_client.list_collections()
                if collections:
                    collection_options = []
                    for col in collections:
                        collection_obj = chroma_client.get_collection(col.name)
                        count = collection_obj.count()
                        collection_options.append((col.name, count))
                    
                    selected_collection = st.selectbox(
                        "ì†ŒìŠ¤ ì»¬ë ‰ì…˜ ì„ íƒ",
                        options=[name for name, count in collection_options],
                        format_func=lambda x: f"{x} ({dict(collection_options)[x]}ê°œ ë¬¸ì„œ)"
                    )
                else:
                    st.error("âŒ ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'ì»¬ë ‰ì…˜ êµ¬ì¶•'ì„ í•´ì£¼ì„¸ìš”.")
                    selected_collection = None
                    
            except Exception as e:
                st.error(f"ì»¬ë ‰ì…˜ í™•ì¸ ì‹¤íŒ¨: {e}")
                selected_collection = None
        
        with col2:
            num_docs = st.number_input(
                "ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜", 
                min_value=5, max_value=100, value=20, step=5,
                help="ë„ˆë¬´ ë§ìœ¼ë©´ API ë¹„ìš©ì´ ì¦ê°€í•©ë‹ˆë‹¤"
            )
            
            questions_per_doc = st.number_input(
                "ë¬¸ì„œë‹¹ ì§ˆë¬¸ ìˆ˜", 
                min_value=1, max_value=3, value=2, step=1
            )
        
        with col3:
            question_types = st.multiselect(
                "ì§ˆë¬¸ ìœ í˜•",
                ["ì‚¬ì‹¤ í™•ì¸", "ë°©ë²• ì„¤ëª…", "ì´ìœ  ì„¤ëª…", "ë¹„êµ", "ì¶”ì²œ"],
                default=["ì‚¬ì‹¤ í™•ì¸", "ë°©ë²• ì„¤ëª…", "ì´ìœ  ì„¤ëª…"],
                help="ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤"
            )
            
            output_filename = st.text_input(
                "ì¶œë ¥ íŒŒì¼ëª…",
                value=f"llm_benchmark_{datetime.now().strftime('%m%d_%H%M')}.json"
            )
        
        # ì˜ˆìƒ ì •ë³´ í‘œì‹œ
        total_questions = num_docs * questions_per_doc
        estimated_cost = total_questions * 0.0001
        
        st.info(f"ğŸ’° **ì˜ˆìƒ ë¹„ìš©**: ${estimated_cost:.4f} | **ì˜ˆìƒ ì§ˆë¬¸ ìˆ˜**: {total_questions}ê°œ")
        
    
        
        # LLM ë²¤ì¹˜ë§ˆí¬ ìƒì„± ì‹¤í–‰
        if st.button("ğŸš€ ì •í™•í•œ ë²¤ì¹˜ë§ˆí¬ ìƒì„±!", use_container_width=True, type="primary"):
            if not selected_collection:
                st.error("âŒ ì»¬ë ‰ì…˜ì„ ì„ íƒí•´ì£¼ì„¸ìš”")
            elif not question_types:
                st.error("âŒ ì§ˆë¬¸ ìœ í˜•ì„ ì„ íƒí•´ì£¼ì„¸ìš”")
            else:
                # ì‹¤ì œ LLM ê¸°ë°˜ ìƒì„± ë¡œì§
                try:
                    from openai import OpenAI
                    openai_client = OpenAI(api_key=api_key)
                    
                    collection = chroma_client.get_collection(selected_collection)
                    
                    # ì§„í–‰ ìƒí™© í‘œì‹œ
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    status_text.info("ë¬¸ì„œ ìƒ˜í”Œë§ ì¤‘...")
                    
                    # ë¬´ì‘ìœ„ë¡œ ë¬¸ì„œ ìƒ˜í”Œë§
                    all_data = collection.get(include=["documents", "metadatas"])
                    total_available = len(all_data['ids'])
                    
                    if total_available < num_docs:
                        st.warning(f"âš ï¸ ìš”ì²­í•œ {num_docs}ê°œë³´ë‹¤ ì ì€ {total_available}ê°œ ë¬¸ì„œë§Œ ìˆìŠµë‹ˆë‹¤.")
                        num_docs = total_available
                    
                    # ëœë¤ ìƒ˜í”Œë§
                    sample_indices = random.sample(range(total_available), num_docs)
                    
                    generated_questions = []
                    
                    for i, idx in enumerate(sample_indices):
                        progress = (i / len(sample_indices)) * 0.9
                        progress_bar.progress(progress)
                        status_text.info(f"ì§ˆë¬¸ ìƒì„± ì¤‘... ({i+1}/{len(sample_indices)})")
                        
                        doc_id = all_data['ids'][idx]
                        content = all_data['documents'][idx]
                        metadata = all_data['metadatas'][idx]
                        
                        # ë„ˆë¬´ ì§§ì€ ë¬¸ì„œëŠ” ê±´ë„ˆë›°ê¸°
                        if len(content.strip()) < 50:
                            continue
                        
                        # ì›ë³¸ ë¬¸ì„œ ID ì¶”ì¶œ (ì²­í‚¹ëœ IDì—ì„œ)
                        original_id = metadata.get('original_id', '_'.join(doc_id.split('_')[:2]))
                        title = metadata.get('title', 'ì œëª© ì—†ìŒ')[:100]
                        
                        # LLMìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±
                        type_prompts = {
                            "ì‚¬ì‹¤ í™•ì¸": "ì´ ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì‚¬ì‹¤ì— ëŒ€í•œ ì§ˆë¬¸",
                            "ë°©ë²• ì„¤ëª…": "ì´ ë¬¸ì„œì—ì„œ ì„¤ëª…í•˜ëŠ” ë°©ë²•ì´ë‚˜ ê³¼ì •ì— ëŒ€í•œ ì§ˆë¬¸",
                            "ì´ìœ  ì„¤ëª…": "ì´ ë¬¸ì„œì—ì„œ ì„¤ëª…í•˜ëŠ” ì´ìœ ë‚˜ ì›ì¸ì— ëŒ€í•œ ì§ˆë¬¸",
                            "ë¹„êµ": "ì´ ë¬¸ì„œì—ì„œ ì–¸ê¸‰ë˜ëŠ” ë¹„êµë‚˜ ì°¨ì´ì ì— ëŒ€í•œ ì§ˆë¬¸",
                            "ì¶”ì²œ": "ì´ ë¬¸ì„œì—ì„œ ì¶”ì²œí•˜ê±°ë‚˜ ì œì•ˆí•˜ëŠ” ê²ƒì— ëŒ€í•œ ì§ˆë¬¸"
                        }
                        
                        selected_types = random.sample(question_types, min(len(question_types), questions_per_doc))
                        
                        prompt = f"""ë‹¤ìŒ ë¸”ë¡œê·¸ ê¸€ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•íˆ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

[ì œëª©]: {title}
[ë‚´ìš©]: {content[:800]}

**ìš”êµ¬ì‚¬í•­:**
1. ì´ ë¬¸ì„œì˜ ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ë§Œ ë§Œë“¤ê¸°
2. ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸ ê° 1ê°œì”© ë§Œë“¤ê¸°:
{chr(10).join(f"   - {type_prompts[t]}" for t in selected_types)}

**ì¶œë ¥ í˜•ì‹:**
ì§ˆë¬¸ë§Œ í•œ ì¤„ì”© ì‘ì„±í•´ì£¼ì„¸ìš”. ë²ˆí˜¸ëŠ” ë¶™ì´ì§€ ë§ˆì„¸ìš”.
ì´ {questions_per_doc}ê°œì˜ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”."""

                        try:
                            response = openai_client.chat.completions.create(
                                model="gpt-4o-mini",
                                messages=[{"role": "user", "content": prompt}],
                                temperature=0.3,
                                max_tokens=300
                            )
                            
                            generated_text = response.choices[0].message.content.strip()
                            
                            for line in generated_text.split('\n'):
                                line = line.strip()
                                if line and len(line) > 10:
                                    # ë²ˆí˜¸ ì œê±°
                                    line = re.sub(r'^\d+[\.\)]\s*', '', line)
                                    line = re.sub(r'^[-\*]\s*', '', line)
                                    
                                    if not line.endswith('?'):
                                        line += '?'
                                    
                                    generated_questions.append({
                                        "query": line,
                                        "relevant_docs": [original_id],
                                        "question_type": "llm_generated",
                                        "source_title": title,
                                        "generation_method": "document_based"
                                    })
                            
                            time.sleep(0.5)  # API ì œí•œ ê³ ë ¤
                            
                        except Exception as e:
                            st.warning(f"ë¬¸ì„œ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:50]}")
                            continue
                    
                    # ê²°ê³¼ ì €ì¥
                    progress_bar.progress(0.95)
                    status_text.info("ê²°ê³¼ ì €ì¥ ì¤‘...")
                    
                    if generated_questions:
                        # ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§
                        unique_questions = []
                        seen_queries = set()
                        
                        for q in generated_questions:
                            query_clean = q['query'].lower().strip()
                            if query_clean not in seen_queries and len(q['query']) > 15:
                                seen_queries.add(query_clean)
                                unique_questions.append(q)
                        
                        # ìµœì¢… ì €ì¥
                        with open(output_filename, 'w', encoding='utf-8') as f:
                            json.dump(unique_questions, f, ensure_ascii=False, indent=2)
                        
                        progress_bar.progress(1.0)
                        status_text.success(f"âœ… ì™„ë£Œ! {len(unique_questions)}ê°œ ì§ˆë¬¸ ìƒì„±ë¨")
                        
                        # ê²°ê³¼ ìš”ì•½
                        col1, col2, col3, col4 = st.columns(4)
                        
                        with col1:
                            st.metric("ìƒì„±ëœ ì§ˆë¬¸", len(unique_questions))
                        with col2:
                            st.metric("ì²˜ë¦¬ëœ ë¬¸ì„œ", len(sample_indices))
                        with col3:
                            actual_cost = len(unique_questions) * 0.0001
                            st.metric("ì‹¤ì œ ë¹„ìš©", f"${actual_cost:.4f}")
                        with col4:
                            avg_per_doc = len(unique_questions) / len(sample_indices)
                            st.metric("ë¬¸ì„œë‹¹ ì§ˆë¬¸", f"{avg_per_doc:.1f}ê°œ")
                        
                        # ìƒ˜í”Œ ì§ˆë¬¸ í‘œì‹œ
                        st.subheader("ğŸ“ ìƒì„±ëœ ì§ˆë¬¸ ìƒ˜í”Œ")
                        
                        sample_qs = random.sample(unique_questions, min(3, len(unique_questions)))
                        for i, q in enumerate(sample_qs, 1):
                            with st.expander(f"ìƒ˜í”Œ ì§ˆë¬¸ {i}: {q['query'][:40]}..."):
                                st.write(f"**ì§ˆë¬¸**: {q['query']}")
                                st.write(f"**ì •ë‹µ ë¬¸ì„œ**: {q['relevant_docs'][0]}")
                                st.write(f"**ì¶œì²˜**: {q.get('source_title', 'N/A')}")
                        
                        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                        with open(output_filename, 'r', encoding='utf-8') as f:
                            file_content = f.read()
                        
                        st.download_button(
                            label="ğŸ“¥ LLM ë²¤ì¹˜ë§ˆí¬ ë‹¤ìš´ë¡œë“œ",
                            data=file_content,
                            file_name=output_filename,
                            mime="application/json"
                        )
                        
                        st.success("""
                        ğŸ¯ **ì´ì œ 'ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰' íƒ­ì—ì„œ ì´ íŒŒì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”!**
                        
                        ì •í™•í•œ ì§ˆë¬¸-ì •ë‹µ ìŒìœ¼ë¡œ ì¸í•´ í›¨ì”¬ ë†’ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                        """)
                        
                    else:
                        st.error("âŒ ì§ˆë¬¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í‚¤ì™€ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                        
                except Exception as e:
                    st.error(f"âŒ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

# --- TAB 3: ì»¬ë ‰ì…˜ êµ¬ì¶• ---
with tab3:
    st.header("ğŸ”§ ChromaDB ì»¬ë ‰ì…˜ êµ¬ì¶•")
    
    if 'json_data' not in st.session_state:
        st.warning("ë¨¼ì € 'ë°ì´í„° ì¤€ë¹„' íƒ­ì—ì„œ JSON ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else:
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ì²­í‚¹ ì„¤ì •")
            chunk_size = st.number_input("ì²­í¬ í¬ê¸°", min_value=100, max_value=2000, value=500, step=100)
            chunk_overlap = st.number_input("ì²­í¬ ê²¹ì¹¨", min_value=0, max_value=200, value=50, step=10)
        
        with col2:
            st.subheader("ëª¨ë¸ ì„ íƒ")
            selected_models = st.multiselect(
                "êµ¬ì¶•í•  ëª¨ë¸ë“¤ì„ ì„ íƒí•˜ì„¸ìš”:",
                options=list(AVAILABLE_MODELS.keys()),
                default=["jhgan/ko-sroberta-multitask", "intfloat/multilingual-e5-base"],
                help="ì—¬ëŸ¬ ëª¨ë¸ì„ ì„ íƒí•˜ë©´ ê°ê° ë³„ë„ ì»¬ë ‰ì…˜ì´ ìƒì„±ë©ë‹ˆë‹¤."
            )
        
        if st.button("ğŸ”§ ì»¬ë ‰ì…˜ êµ¬ì¶• ì‹œì‘!", use_container_width=True, type="primary"):
            if selected_models:
                total_models = len(selected_models)
                main_progress = st.progress(0, text="ì»¬ë ‰ì…˜ êµ¬ì¶• ì‹œì‘...")
                
                for i, model_name in enumerate(selected_models):
                    collection_name = AVAILABLE_MODELS[model_name]
                    st.subheader(f"[{i+1}/{total_models}] {model_name}")
                    
                    try:
                        build_vector_database(
                            st.session_state['json_data'], 
                            model_name, 
                            collection_name,
                            chunk_size,
                            chunk_overlap
                        )
                    except Exception as e:
                        st.error(f"âŒ {model_name} êµ¬ì¶• ì‹¤íŒ¨: {e}")
                    
                    main_progress.progress((i + 1) / total_models, 
                                         text=f"ì§„í–‰ ìƒí™©: {i+1}/{total_models} ì™„ë£Œ")
                
                st.success("ğŸ‰ ëª¨ë“  ì»¬ë ‰ì…˜ êµ¬ì¶• ì™„ë£Œ!")
            else:
                st.warning("ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

# --- TAB 4: ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ---
with tab4:
    st.header("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
    
    try:
        chroma_client = get_chroma_client()
        existing_collections = [col.name for col in chroma_client.list_collections()]
        available_models = {k: v for k, v in AVAILABLE_MODELS.items() if v in existing_collections}
        
        if not available_models:
            st.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. 'ì»¬ë ‰ì…˜ êµ¬ì¶•' íƒ­ì—ì„œ ë¨¼ì € êµ¬ì¶•í•´ì£¼ì„¸ìš”.")
        else:
            st.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜: {len(available_models)}ê°œ")
            st.table(pd.DataFrame(list(available_models.items()), columns=['Model', 'Collection']))
            
    except Exception as e:
        st.error(f"ChromaDB ì—°ê²° ì‹¤íŒ¨: {e}")
        available_models = {}
    
    if available_models:
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ë²¤ì¹˜ë§ˆí¬ ì„¤ì •")
            k_value = st.slider("ìµœì¢… ìƒìœ„ Kê°œ ê²°ê³¼", 1, 20, 5)
            use_reranker = st.checkbox("Re-ranking ì‚¬ìš©", value=True)
            initial_k = st.number_input("1ì°¨ ê²€ìƒ‰ ê°œìˆ˜", min_value=k_value, max_value=50, value=25, disabled=not use_reranker)
            use_batch = st.checkbox("ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš©", value=True, help="GPU ê°€ì†")
            calculate_detailed = st.checkbox("ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚°", value=True, help="MRR, NDCG ë“±")
        
        with col2:
            st.subheader("ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°")
            
            # LLM ìƒì„± íŒŒì¼ ìë™ ê°ì§€
            try:
                llm_files = [f for f in os.listdir('.') if f.startswith('llm_benchmark_') and f.endswith('.json')]
            except:
                llm_files = []
            
            if llm_files:
                st.success(f"ğŸ¤– LLM ìƒì„± íŒŒì¼ ë°œê²¬: {len(llm_files)}ê°œ")
                selected_llm_file = st.selectbox("LLM ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ ì„ íƒ:", llm_files)
                benchmark_file_path = selected_llm_file
            else:
                st.warning("ğŸ’¡ LLM ìƒì„± íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. 'LLM ì •í™•í•œ ë²¤ì¹˜ë§ˆí¬' íƒ­ì—ì„œ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")
                benchmark_file_path = st.text_input("ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ ê²½ë¡œ (ì§ì ‘ ì…ë ¥)", value="")
            
            if benchmark_file_path and os.path.exists(benchmark_file_path):
                with open(benchmark_file_path, 'r', encoding='utf-8') as f:
                    benchmark_data = json.load(f)
                
                # íŒŒì¼ ìœ í˜• í™•ì¸
                is_llm_generated = any(item.get('generation_method') == 'document_based' for item in benchmark_data)
                
                if is_llm_generated:
                    st.success(f"âœ… LLM ì •í™•í•œ ë²¤ì¹˜ë§ˆí¬ ë¡œë“œë¨ ({len(benchmark_data)}ê°œ ì§ˆë¬¸)")
                    st.info("ğŸ¯ **ë†’ì€ ì •í™•ë„ì˜ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!**")
                else:
                    st.warning(f"âš ï¸ ê¸°ì¡´ ë°©ì‹ ë²¤ì¹˜ë§ˆí¬ ë¡œë“œë¨ ({len(benchmark_data)}ê°œ ì§ˆë¬¸)")
                    st.info("ğŸ’¡ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ìœ„í•´ LLM ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
                
                if benchmark_data:
                    type_counts = {}
                    difficulty_counts = {}
                    for item in benchmark_data:
                        q_type = item.get('question_type', 'unknown')
                        difficulty = item.get('difficulty', 'basic')
                        type_counts[q_type] = type_counts.get(q_type, 0) + 1
                        difficulty_counts[difficulty] = difficulty_counts.get(difficulty, 0) + 1
                    
                    st.write("**ì§ˆë¬¸ ìœ í˜• ë¶„í¬:**")
                    for q_type, count in type_counts.items():
                        emoji = "ğŸ¤–" if q_type == "llm_generated" else "ğŸ”¥" if q_type in ['comparison', 'reasoning', 'complex'] else "ğŸ“"
                        st.write(f"- {emoji} {q_type}: {count}ê°œ")
                    
                    # Re-ranker íš¨ê³¼ ì˜ˆìƒì¹˜ ê³„ì‚°
                    if is_llm_generated:
                        st.success("ğŸ¯ **LLM ìƒì„±**: Re-ranker íš¨ê³¼ ì •í™•í•˜ê²Œ ì¸¡ì • ê°€ëŠ¥!")
                    else:
                        complex_count = sum(difficulty_counts.get('advanced', 0) for _ in range(1))
                        if len(benchmark_data) > 0:
                            complexity_ratio = complex_count / len(benchmark_data) * 100
                            st.info(f"ğŸ¯ Re-ranker íš¨ê³¼ ì˜ˆìƒ: {complexity_ratio:.1f}% (ë³µì¡í•œ ì§ˆë¬¸ ë¹„ìœ¨)")
                        
                        # ì§ˆë¬¸ ë‚œì´ë„ë³„ ë¶„í¬ ë¯¸ë‹ˆ ì°¨íŠ¸
                        if len(difficulty_counts) > 1:
                            diff_df = pd.DataFrame(list(difficulty_counts.items()), 
                                                 columns=['Difficulty', 'Count'])
                            fig_mini = px.bar(diff_df, x='Difficulty', y='Count', 
                                            height=200, title='ë‚œì´ë„ ë¶„í¬')
                            st.plotly_chart(fig_mini, use_container_width=True)
                        
            else:
                if benchmark_file_path:
                    st.error(f"ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ '{benchmark_file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.warning("ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ì„ ì„ íƒí•˜ê±°ë‚˜ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                if not llm_files:
                    st.info("'LLM ì •í™•í•œ ë²¤ì¹˜ë§ˆí¬' íƒ­ì—ì„œ ë¨¼ì € ì •í™•í•œ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ì„¸ìš”.")
                benchmark_data = None
        
        if st.button("ğŸš€ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰!", use_container_width=True, type="primary") and benchmark_data:
            progress_bar = st.progress(0, text="ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
            results_placeholder = st.empty()
            benchmark_results = []
            detailed_metrics_results = {}
            
            reranker = load_reranker_model() if use_reranker else None
            total_models = len(available_models)
            
            for i, (model_name, collection_name) in enumerate(available_models.items()):
                progress_bar.progress(i / total_models, text=f"[{i+1}/{total_models}] {model_name.split('/')[-1]}")
                
                try:
                    retriever_model = load_embedding_model(model_name)
                    collection = chroma_client.get_collection(name=collection_name)
                    
                    hit_count = 0
                    total_mrr = 0
                    total_ndcg = 0
                    total_precision = 0
                    
                    if use_batch:
                        queries = [item['query'] for item in benchmark_data]
                        n_results = initial_k if use_reranker else k_value
                        query_embeddings = retriever_model.encode(queries, batch_size=32, show_progress_bar=False)
                        
                        for idx, (item, embedding) in enumerate(zip(benchmark_data, query_embeddings)):
                            results = collection.query(
                                query_embeddings=[embedding.tolist()],
                                n_results=n_results,
                                include=["documents"]
                            )
                            
                            retrieved_ids = results['ids'][0]
                            
                            if use_reranker and reranker:
                                retrieved_docs = results['documents'][0]
                                pairs = [[item['query'], doc] for doc in retrieved_docs]
                                scores = reranker.predict(pairs)
                                reranked = sorted(zip(scores, retrieved_ids), key=lambda x: x[0], reverse=True)
                                final_ids = [doc_id for _, doc_id in reranked[:k_value]]
                            else:
                                final_ids = retrieved_ids[:k_value]
                            
                            relevant_docs = set(item['relevant_docs'])
                            is_hit = any(
                                r_id.startswith(ans_id + "_") 
                                for r_id in final_ids 
                                for ans_id in relevant_docs
                            )
                            
                            if is_hit:
                                hit_count += 1
                            
                            if calculate_detailed:
                                metrics = calculate_detailed_metrics(final_ids, relevant_docs, k_value)
                                total_mrr += metrics['mrr']
                                total_ndcg += metrics['ndcg']
                                total_precision += metrics['precision_at_k']
                    else:
                        for item in benchmark_data:
                            query = item['query']
                            relevant_docs = set(item['relevant_docs'])
                            
                            n_results = initial_k if use_reranker else k_value
                            query_embedding = retriever_model.encode(query).tolist()
                            results = collection.query(
                                query_embeddings=[query_embedding],
                                n_results=n_results,
                                include=["documents"]
                            )
                            
                            retrieved_ids = results['ids'][0]
                            
                            if use_reranker and reranker:
                                retrieved_docs = results['documents'][0]
                                pairs = [[query, doc] for doc in retrieved_docs]
                                scores = reranker.predict(pairs)
                                reranked = sorted(zip(scores, retrieved_ids), key=lambda x: x[0], reverse=True)
                                final_ids = [doc_id for _, doc_id in reranked[:k_value]]
                            else:
                                final_ids = retrieved_ids[:k_value]
                            
                            is_hit = any(
                                r_id.startswith(ans_id + "_") 
                                for r_id in final_ids 
                                for ans_id in relevant_docs
                            )
                            
                            if is_hit:
                                hit_count += 1
                            
                            if calculate_detailed:
                                metrics = calculate_detailed_metrics(final_ids, relevant_docs, k_value)
                                total_mrr += metrics['mrr']
                                total_ndcg += metrics['ndcg']
                                total_precision += metrics['precision_at_k']
                    
                    hit_rate = hit_count / len(benchmark_data)
                    avg_mrr = total_mrr / len(benchmark_data) if calculate_detailed else 0
                    avg_ndcg = total_ndcg / len(benchmark_data) if calculate_detailed else 0
                    avg_precision = total_precision / len(benchmark_data) if calculate_detailed else 0
                    
                    result_entry = {
                        "Model": model_name,
                        "Re-ranked": "Yes" if use_reranker else "No", 
                        "Batch": "Yes" if use_batch else "No",
                        f"Hit Rate @{k_value}": f"{hit_rate:.2%}"
                    }
                    
                    if calculate_detailed:
                        result_entry.update({
                            "MRR": f"{avg_mrr:.3f}",
                            "NDCG": f"{avg_ndcg:.3f}",
                            "Precision@K": f"{avg_precision:.3f}"
                        })
                        detailed_metrics_results[model_name] = [hit_rate, avg_mrr, avg_ndcg, avg_precision]
                    
                    benchmark_results.append(result_entry)
                    
                except Exception as e:
                    error_entry = {
                        "Model": model_name,
                        "Re-ranked": "Yes" if use_reranker else "No",
                        "Batch": "Yes" if use_batch else "No", 
                        f"Hit Rate @{k_value}": f"ì˜¤ë¥˜: {str(e)[:50]}"
                    }
                    benchmark_results.append(error_entry)
                
                results_placeholder.dataframe(pd.DataFrame(benchmark_results), use_container_width=True)
            
            progress_bar.progress(1.0, text="âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
            
            # ì„¸ì…˜ ìƒíƒœì— ê²°ê³¼ ì €ì¥ (ê·¸ë˜í”„ íƒ­ì—ì„œ ì‚¬ìš©)
            st.session_state['benchmark_results'] = benchmark_results
            st.session_state['detailed_metrics_results'] = detailed_metrics_results
            
            # ê°„ë‹¨í•œ ì‹œê°í™”
            if len(benchmark_results) > 1:
                st.subheader("ğŸ“Š ê²°ê³¼ ì‹œê°í™”")
                try:
                    fig = create_model_comparison_chart(benchmark_results)
                    if fig is not None:
                        st.plotly_chart(fig, use_container_width=True, key="model_comparison_chart")
                except Exception as e:
                    st.error(f"ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            
            if st.button("ğŸ“ ê²°ê³¼ ì €ì¥"):
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"benchmark_results_{timestamp}.csv"
                pd.DataFrame(benchmark_results).to_csv(filename, index=False, encoding='utf-8-sig')
                st.success(f"ê²°ê³¼ê°€ '{filename}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

        # ë””ë²„ê¹… ë° ë¬¸ì œ í•´ê²° ì„¹ì…˜
        st.markdown("---")
        st.subheader("ğŸ”§ ë””ë²„ê¹… ë° ë¬¸ì œ í•´ê²°")
        
        with st.expander("ğŸš¨ ì„±ëŠ¥ì´ ë‚®ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”"):
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ğŸ¥ ì»¬ë ‰ì…˜ ìƒíƒœ ì²´í¬"):
                    health_df = check_collection_health()
                    if health_df is not None:
                        st.session_state['collections_health'] = health_df
                
                if st.button("ğŸ§ª ê°„ë‹¨ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"):
                    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                    simple_queries = ["ëª¨ê¸°", "ì œí’ˆ", "íš¨ê³¼", "ì‚¬ìš©", "ì¶”ì²œ"]
                    
                    try:
                        test_results = []
                        for model_name, collection_name in list(available_models.items())[:3]:
                            retriever_model = load_embedding_model(model_name)
                            collection = chroma_client.get_collection(collection_name)
                            
                            hits = 0
                            for query in simple_queries:
                                query_embedding = retriever_model.encode(query).tolist()
                                results = collection.query(query_embeddings=[query_embedding], n_results=5)
                                
                                # ë‹¨ìˆœ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
                                docs = results.get('documents', [[]])[0]
                                if any(query in str(doc) for doc in docs):
                                    hits += 1
                            
                            hit_rate = hits / len(simple_queries) * 100
                            test_results.append({
                                'Model': model_name.split('/')[-1],
                                'Simple Hit Rate': f"{hit_rate:.1f}%"
                            })
                        
                        st.dataframe(pd.DataFrame(test_results))
                        
                        # í‰ê·  ì„±ëŠ¥ ë¶„ì„
                        rates = [float(r['Simple Hit Rate'].replace('%', '')) for r in test_results if '%' in r['Simple Hit Rate']]
                        if rates:
                            avg = sum(rates) / len(rates)
                            if avg > 70:
                                st.success(f"âœ… ê¸°ë³¸ ê²€ìƒ‰ ê¸°ëŠ¥ ì •ìƒ (í‰ê·  {avg:.1f}%)")
                            elif avg > 30:
                                st.warning(f"âš ï¸ ê¸°ë³¸ ê²€ìƒ‰ ê¸°ëŠ¥ ë³´í†µ (í‰ê·  {avg:.1f}%)")
                            else:
                                st.error(f"âŒ ê¸°ë³¸ ê²€ìƒ‰ ê¸°ëŠ¥ ë¶ˆëŸ‰ (í‰ê·  {avg:.1f}%) - ì»¬ë ‰ì…˜ ì¬êµ¬ì¶• í•„ìš”")
                        
                    except Exception as e:
                        st.error(f"ê°„ë‹¨ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            with col2:
                if st.button("ğŸ” ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ê²€ì¦"):
                    # LLM íŒŒì¼ ë¨¼ì € í™•ì¸
                    llm_files = [f for f in os.listdir('.') if f.startswith('llm_benchmark_') and f.endswith('.json')]
                    
                    if llm_files:
                        latest_llm_file = max(llm_files, key=lambda x: os.path.getmtime(x))
                        with open(latest_llm_file, 'r', encoding='utf-8') as f:
                            benchmark_data = json.load(f)
                        
                        st.write(f"ğŸ“Š LLM ë²¤ì¹˜ë§ˆí¬ íŒŒì¼: {latest_llm_file}")
                        st.write(f"ğŸ“Š ì´ ì§ˆë¬¸ ìˆ˜: {len(benchmark_data)}")
                        
                        # LLM ìƒì„± ì—¬ë¶€ í™•ì¸
                        is_llm = any(item.get('generation_method') == 'document_based' for item in benchmark_data)
                        if is_llm:
                            st.success("âœ… LLM ê¸°ë°˜ ì •í™•í•œ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤!")
                        
                        # ì°¸ì¡° ë¬¸ì„œ ID ë¶„ì„
                        all_ref_ids = set()
                        for item in benchmark_data:
                            all_ref_ids.update(item.get('relevant_docs', []))
                        
                        st.write(f"ğŸ“‹ ì°¸ì¡° ë¬¸ì„œ ID ìˆ˜: {len(all_ref_ids)}")
                        st.write(f"ğŸ“ ìƒ˜í”Œ ID: {list(all_ref_ids)[:5]}")
                        
                        # ì»¬ë ‰ì…˜ê³¼ì˜ ì¼ì¹˜ë„ í™•ì¸
                        if available_models:
                            first_collection_name = list(available_models.values())[0]
                            collection = chroma_client.get_collection(first_collection_name)
                            
                            # ìƒ˜í”Œ ê²€ì¦
                            sample_ids = list(all_ref_ids)[:5]
                            found_count = 0
                            
                            try:
                                all_collection_data = collection.get(include=['ids'])
                                collection_ids = all_collection_data.get('ids', [])
                                
                                for ref_id in sample_ids:
                                    matching_chunks = [cid for cid in collection_ids if cid.startswith(ref_id + "_")]
                                    if matching_chunks:
                                        found_count += 1
                                
                                match_ratio = found_count / len(sample_ids) * 100
                                
                                if match_ratio > 80:
                                    st.success(f"âœ… ë°ì´í„° ì¼ì¹˜ë„ ìš°ìˆ˜ ({match_ratio:.1f}%)")
                                elif match_ratio > 40:
                                    st.warning(f"âš ï¸ ë°ì´í„° ì¼ì¹˜ë„ ë³´í†µ ({match_ratio:.1f}%)")
                                else:
                                    st.error(f"âŒ ë°ì´í„° ë¶ˆì¼ì¹˜ ì‹¬ê° ({match_ratio:.1f}%) - ì»¬ë ‰ì…˜ ì¬êµ¬ì¶• í•„ìš”")
                                    
                            except Exception as e:
                                st.error(f"ì¼ì¹˜ë„ í™•ì¸ ì‹¤íŒ¨: {e}")
                    else:
                        st.error("LLM ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.")
                
                # ê°œë³„ ëª¨ë¸ ë””ë²„ê¹…
                if available_models:
                    selected_debug_model = st.selectbox("ë””ë²„ê¹…í•  ëª¨ë¸:", list(available_models.keys()), key="debug_model")
                    
                    if st.button("ğŸ” ì„ íƒ ëª¨ë¸ ìƒì„¸ ë””ë²„ê¹…"):
                        # LLM íŒŒì¼ ì°¾ê¸°
                        llm_files = [f for f in os.listdir('.') if f.startswith('llm_benchmark_') and f.endswith('.json')]
                        
                        if llm_files:
                            latest_llm_file = max(llm_files, key=lambda x: os.path.getmtime(x))
                            with open(latest_llm_file, 'r', encoding='utf-8') as f:
                                benchmark_data = json.load(f)
                            
                            debug_benchmark_performance(
                                selected_debug_model,
                                available_models[selected_debug_model],
                                benchmark_data[:3]  # ì²˜ìŒ 3ê°œë§Œ ë””ë²„ê¹…
                            )
                        else:
                            st.error("LLM ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë¬¸ì œ í•´ê²° ê°€ì´ë“œ
            st.markdown("---")
            st.subheader("ğŸ’¡ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ")
            
            st.markdown("""
            **ğŸ”¥ ì„±ëŠ¥ì´ ë§¤ìš° ë‚®ì€ ê²½ìš° (10% ë¯¸ë§Œ)**:
            1. **LLM ë²¤ì¹˜ë§ˆí¬ ì‚¬ìš©**: íƒ­2ì—ì„œ ì •í™•í•œ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ìƒì„±
            2. **ì»¬ë ‰ì…˜ ì¬êµ¬ì¶•**: íƒ­3ì—ì„œ ì²­í¬ í¬ê¸°ë¥¼ ëŠ˜ë ¤ì„œ ì¬êµ¬ì¶• (500â†’1000)
            3. **ë°ì´í„° ì¼ì¹˜ë„ í™•ì¸**: ìœ„ì˜ "ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ê²€ì¦" ì‹¤í–‰
            
            **âš ï¸ ì¼ë¶€ ëª¨ë¸ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ê²½ìš°**:
            1. **GPU ë©”ëª¨ë¦¬ ë¶€ì¡±**: ë°°ì¹˜ ì²˜ë¦¬ ë„ê¸° ë˜ëŠ” ëª¨ë¸ í•˜ë‚˜ì”© í…ŒìŠ¤íŠ¸
            2. **ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨**: ì¸í„°ë„· ì—°ê²° í™•ì¸ ë˜ëŠ” ëª¨ë¸ëª… í™•ì¸
            3. **ì»¬ë ‰ì…˜ ëˆ„ë½**: "ì»¬ë ‰ì…˜ ìƒíƒœ ì²´í¬"ë¡œ í™•ì¸
            
            **ğŸ“Š Re-ranker íš¨ê³¼ê°€ ì—†ëŠ” ê²½ìš°**:
            1. **LLM ë²¤ì¹˜ë§ˆí¬ ì‚¬ìš©**: ì •í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ Re-ranker íš¨ê³¼ ì œëŒ€ë¡œ ì¸¡ì •
            2. **Kê°’ì´ ë„ˆë¬´ ì‘ìŒ**: 1ì°¨ ê²€ìƒ‰ì„ 50ê°œ ì´ìƒìœ¼ë¡œ ëŠ˜ë¦¬ê¸°
            3. **ê¸°ë³¸ ì„±ëŠ¥ì´ ì´ë¯¸ ì¢‹ìŒ**: ì •ìƒì ì¸ í˜„ìƒì¼ ìˆ˜ ìˆìŒ
            """)

# --- TAB 5: ì„±ëŠ¥ ë¶„ì„ ---
with tab5:
    st.header("ğŸ“ˆ ìƒì„¸ ì„±ëŠ¥ ë¶„ì„ ë° ì‹œê°í™”")
    
    if 'benchmark_results' not in st.session_state:
        st.warning("ë¨¼ì € 'ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰' íƒ­ì—ì„œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    else:
        benchmark_results = st.session_state['benchmark_results']
        detailed_metrics = st.session_state.get('detailed_metrics_results', {})
        
        # ë¶„ì„ ìœ í˜• ì„ íƒ
        analysis_type = st.selectbox(
            "ë¶„ì„ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”:",
            ["ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ", "Re-ranker íš¨ê³¼ ë¶„ì„", "Kê°’ ë³€í™” ë¶„ì„", "ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„ì„", "ì¢…í•© ëŒ€ì‹œë³´ë“œ"]
        )
        
        if analysis_type == "ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ":
            st.subheader("ğŸ† ëª¨ë¸ë³„ ì„±ëŠ¥ ìˆœìœ„")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # ë§‰ëŒ€ ì°¨íŠ¸
                fig_bar = create_model_comparison_chart(benchmark_results)
                st.plotly_chart(fig_bar, use_container_width=True)
            
            with col2:
                # ìƒì„¸ ë©”íŠ¸ë¦­ ë ˆì´ë” ì°¨íŠ¸ (ìˆëŠ” ê²½ìš°)
                if detailed_metrics:
                    fig_radar = create_detailed_metrics_radar_chart(detailed_metrics)
                    st.plotly_chart(fig_radar, use_container_width=True)
                else:
                    st.info("ìƒì„¸ ë©”íŠ¸ë¦­ì„ ë³´ë ¤ë©´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹œ 'ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚°'ì„ ì²´í¬í•˜ì„¸ìš”.")
            
            # ì„±ëŠ¥ ìˆœìœ„ í…Œì´ë¸”
            df_results = pd.DataFrame(benchmark_results)
            hit_rate_col = [col for col in df_results.columns if "Hit Rate" in col][0]
            df_results['hit_rate_numeric'] = df_results[hit_rate_col].str.replace('%', '').str.replace('ì˜¤ë¥˜.*', '0', regex=True).astype(float)
            df_sorted = df_results.sort_values('hit_rate_numeric', ascending=False)
            
            st.subheader("ğŸ“‹ ì„±ëŠ¥ ìˆœìœ„í‘œ")
            st.dataframe(df_sorted[['Model', hit_rate_col, 'Re-ranked', 'Batch']], use_container_width=True)
            
        elif analysis_type == "Re-ranker íš¨ê³¼ ë¶„ì„":
            st.subheader("ğŸ”„ Re-ranker íš¨ê³¼ ë¶„ì„")
            
            # Re-ranker ì‚¬ìš©/ë¯¸ì‚¬ìš© ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            reranker_yes = [r for r in benchmark_results if r.get('Re-ranked') == 'Yes']
            reranker_no = [r for r in benchmark_results if r.get('Re-ranked') == 'No']
            
            if reranker_yes and reranker_no:
                # ë¹„êµ ì°¨íŠ¸ ìƒì„±
                comparison_data = []
                for model in set([r['Model'] for r in benchmark_results]):
                    yes_result = next((r for r in reranker_yes if r['Model'] == model), None)
                    no_result = next((r for r in reranker_no if r['Model'] == model), None)
                    
                    if yes_result and no_result:
                        hit_rate_col = [col for col in yes_result.keys() if "Hit Rate" in col][0]
                        comparison_data.append({
                            'Model': model.split('/')[-1],
                            'Without_Reranker': float(no_result[hit_rate_col].replace('%', '')),
                            'With_Reranker': float(yes_result[hit_rate_col].replace('%', ''))
                        })
                
                if comparison_data:
                    fig_comparison = create_reranker_comparison_chart(comparison_data)
                    st.plotly_chart(fig_comparison, use_container_width=True)
                    
                    # ê°œì„  íš¨ê³¼ ê³„ì‚°
                    improvements = []
                    for data in comparison_data:
                        improvement = data['With_Reranker'] - data['Without_Reranker']
                        improvements.append({
                            'Model': data['Model'],
                            'Improvement': f"{improvement:+.2f}%"
                        })
                    
                    st.subheader("ğŸ“Š Re-ranker ê°œì„  íš¨ê³¼")
                    st.dataframe(pd.DataFrame(improvements), use_container_width=True)
                else:
                    st.info("Re-ranker ë¹„êµ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                st.info("Re-ranker ì‚¬ìš©/ë¯¸ì‚¬ìš© ë¹„êµë¥¼ ìœ„í•´ì„œëŠ” ë‘ ê°€ì§€ ì„¤ì •ìœ¼ë¡œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")
                
        elif analysis_type == "Kê°’ ë³€í™” ë¶„ì„":
            st.subheader("ğŸ“ˆ Kê°’ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ì‹œë®¬ë ˆì´ì…˜")
            
            st.info("ì´ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë‹¤ì–‘í•œ Kê°’ìœ¼ë¡œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")
            
            # LLM ë²¤ì¹˜ë§ˆí¬ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
            sample_k_analysis = {}
            for model in [r['Model'] for r in benchmark_results[:3]]:
                k_values = list(range(1, 21))
                # LLM ë²¤ì¹˜ë§ˆí¬ì—ì„œëŠ” ë” ë†’ì€ ê¸°ë³¸ ì„±ëŠ¥ì„ ì‹œë®¬ë ˆì´ì…˜
                base_rate = 0.6  # LLM ê¸°ë°˜ì´ë¯€ë¡œ ë” ë†’ì€ ì‹œì‘ì 
                hit_rates = [min(0.95, base_rate + (k-1) * 0.02 + random.uniform(-0.03, 0.03)) for k in k_values]
                sample_k_analysis[model] = {'k_values': k_values, 'hit_rates': [h*100 for h in hit_rates]}
            
            fig_k_analysis = create_k_value_analysis_chart(sample_k_analysis)
            st.plotly_chart(fig_k_analysis, use_container_width=True)
            
            st.info("ğŸ’¡ LLM ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‚¬ìš©í•˜ë©´ ë” ì •í™•í•œ Kê°’ ìµœì í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                
        elif analysis_type == "ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„ì„":
            st.subheader("ğŸ¯ ì§ˆë¬¸ ìœ í˜•ë³„ ì„±ëŠ¥ ë¶„ì„")
            
            # LLM ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ ìš°ì„  í™•ì¸
            llm_files = [f for f in os.listdir('.') if f.startswith('llm_benchmark_') and f.endswith('.json')]
            
            if llm_files:
                latest_llm_file = max(llm_files, key=lambda x: os.path.getmtime(x))
                
                with open(latest_llm_file, 'r', encoding='utf-8') as f:
                    benchmark_data = json.load(f)
                
                st.success(f"âœ… LLM ë²¤ì¹˜ë§ˆí¬ ë¶„ì„: {latest_llm_file} ({len(benchmark_data)}ê°œ ì§ˆë¬¸)")
                
                # ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬ ë° ë³µì¡ë„ ë¶„ì„
                type_counts = {}
                difficulty_counts = {}
                complexity_analysis = {}
                
                for item in benchmark_data:
                    q_type = item.get('question_type', 'unknown')
                    difficulty = item.get('difficulty', 'basic')
                    relevant_docs_count = len(item.get('relevant_docs', []))
                    
                    type_counts[q_type] = type_counts.get(q_type, 0) + 1
                    difficulty_counts[difficulty] = difficulty_counts.get(difficulty, 0) + 1
                    
                    if q_type not in complexity_analysis:
                        complexity_analysis[q_type] = {'total': 0, 'multi_doc': 0, 'advanced': 0}
                    
                    complexity_analysis[q_type]['total'] += 1
                    if relevant_docs_count > 1:
                        complexity_analysis[q_type]['multi_doc'] += 1
                    if difficulty == 'advanced':
                        complexity_analysis[q_type]['advanced'] += 1
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    # ì§ˆë¬¸ ìœ í˜• ë¶„í¬ íŒŒì´ ì°¨íŠ¸
                    type_df = pd.DataFrame(list(type_counts.items()), columns=['Question Type', 'Count'])
                    fig_pie = px.pie(type_df, values='Count', names='Question Type', 
                                   title='ì§ˆë¬¸ ìœ í˜• ë¶„í¬')
                    st.plotly_chart(fig_pie, use_container_width=True)
                
                with col2:
                    # ë‚œì´ë„ ë¶„í¬ ë§‰ëŒ€ ì°¨íŠ¸
                    difficulty_df = pd.DataFrame(list(difficulty_counts.items()), 
                                               columns=['Difficulty', 'Count'])
                    fig_bar = px.bar(difficulty_df, x='Difficulty', y='Count', 
                                   title='ë‚œì´ë„ ë¶„í¬', color='Count',
                                   color_discrete_sequence=['lightcoral', 'lightblue'])
                    st.plotly_chart(fig_bar, use_container_width=True)
                
                with col3:
                    # ë³µì¡ë„ ë¶„ì„ ì°¨íŠ¸
                    complexity_data = []
                    for q_type, data in complexity_analysis.items():
                        complexity_score = (data['multi_doc'] + data['advanced']) / data['total'] * 100
                        complexity_data.append({
                            'Question Type': q_type,
                            'Complexity Score': complexity_score
                        })
                    
                    complexity_df = pd.DataFrame(complexity_data)
                    fig_complexity = px.bar(complexity_df, x='Question Type', y='Complexity Score',
                                          title='ì§ˆë¬¸ ìœ í˜•ë³„ ë³µì¡ë„', 
                                          color='Complexity Score',
                                          color_continuous_scale='Reds')
                    fig_complexity.update_layout(xaxis_tickangle=-45)
                    st.plotly_chart(fig_complexity, use_container_width=True)
                
                # ìƒì„¸ ë¶„ì„ í…Œì´ë¸”
                st.subheader("ğŸ“‹ ì§ˆë¬¸ ìœ í˜•ë³„ ìƒì„¸ ë¶„ì„")
                
                analysis_table = []
                for q_type, data in complexity_analysis.items():
                    total = data['total']
                    multi_doc_ratio = data['multi_doc'] / total * 100 if total > 0 else 0
                    advanced_ratio = data['advanced'] / total * 100 if total > 0 else 0
                    complexity_score = (data['multi_doc'] + data['advanced']) / total * 100 if total > 0 else 0
                    
                    # Re-ranker íš¨ê³¼ ì˜ˆìƒ
                    if complexity_score > 50:
                        reranker_effect = "ë†’ìŒ ğŸ”¥"
                    elif complexity_score > 25:
                        reranker_effect = "ì¤‘ê°„ âš¡"
                    else:
                        reranker_effect = "ë‚®ìŒ ğŸ“"
                    
                    analysis_table.append({
                        'ì§ˆë¬¸ ìœ í˜•': q_type,
                        'ê°œìˆ˜': total,
                        'ë¹„ìœ¨': f"{total/len(benchmark_data)*100:.1f}%",
                        'ë©€í‹° ë¬¸ì„œ ë¹„ìœ¨': f"{multi_doc_ratio:.1f}%",
                        'ê³ ê¸‰ ë‚œì´ë„ ë¹„ìœ¨': f"{advanced_ratio:.1f}%",
                        'ë³µì¡ë„ ì ìˆ˜': f"{complexity_score:.1f}%",
                        'Re-ranker íš¨ê³¼ ì˜ˆìƒ': reranker_effect
                    })
                
                st.dataframe(pd.DataFrame(analysis_table), use_container_width=True)
                
                # ì¸ì‚¬ì´íŠ¸ ì œê³µ
                st.subheader("ğŸ’¡ ë¶„ì„ ì¸ì‚¬ì´íŠ¸")
                
                high_complexity_types = [item for item in analysis_table if float(item['ë³µì¡ë„ ì ìˆ˜'].replace('%', '')) > 50]
                if high_complexity_types:
                    st.success(f"ğŸ¯ **Re-ranker íš¨ê³¼ê°€ í´ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ì§ˆë¬¸ ìœ í˜•**: {', '.join([item['ì§ˆë¬¸ ìœ í˜•'] for item in high_complexity_types])}")
                
                total_advanced = sum(data['advanced'] for data in complexity_analysis.values())
                advanced_ratio = total_advanced / len(benchmark_data) * 100
                
                if advanced_ratio > 60:
                    st.info(f"ğŸ”¥ **ê³ ê¸‰ ì§ˆë¬¸ ë¹„ìœ¨ì´ ë†’ìŒ** ({advanced_ratio:.1f}%): Re-ranker ì„±ëŠ¥ í–¥ìƒì´ í¬ê²Œ ë‚˜íƒ€ë‚  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.")
                elif advanced_ratio > 30:
                    st.info(f"âš¡ **ì¤‘ê°„ ìˆ˜ì¤€ì˜ ê³ ê¸‰ ì§ˆë¬¸** ({advanced_ratio:.1f}%): Re-ranker íš¨ê³¼ê°€ ì ë‹¹íˆ ë‚˜íƒ€ë‚  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.")
                else:
                    st.warning(f"ğŸ“ **ê¸°ë³¸ ì§ˆë¬¸ ìœ„ì£¼** ({advanced_ratio:.1f}%): Re-ranker íš¨ê³¼ê°€ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ë³µì¡í•œ ì§ˆë¬¸ ìƒì„±ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
            else:
                st.warning("LLM ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'LLM ì •í™•í•œ ë²¤ì¹˜ë§ˆí¬' íƒ­ì—ì„œ ìƒì„±í•´ì£¼ì„¸ìš”.")
                
        elif analysis_type == "ì¢…í•© ëŒ€ì‹œë³´ë“œ":
            st.subheader("ğŸ›ï¸ ì¢…í•© ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ")
            
            # ë©”íŠ¸ë¦­ ìš”ì•½
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                avg_hit_rate = np.mean([
                    float(r[next(col for col in r.keys() if "Hit Rate" in col)].replace('%', '')) 
                    for r in benchmark_results if '%' in str(r.get(next(col for col in r.keys() if "Hit Rate" in col), ''))
                ])
                st.metric("í‰ê·  Hit Rate", f"{avg_hit_rate:.1f}%")
            
            with col2:
                total_models = len([r for r in benchmark_results if 'Model' in r])
                st.metric("í…ŒìŠ¤íŠ¸ëœ ëª¨ë¸ ìˆ˜", total_models)
            
            with col3:
                reranker_models = len([r for r in benchmark_results if r.get('Re-ranked') == 'Yes'])
                st.metric("Re-ranker ì‚¬ìš© ëª¨ë¸", reranker_models)
            
            with col4:
                gpu_models = len([r for r in benchmark_results if r.get('Batch') == 'Yes'])
                st.metric("GPU ë°°ì¹˜ ì²˜ë¦¬", gpu_models)
            
            # ì„±ëŠ¥ íˆíŠ¸ë§µ
            if detailed_metrics:
                st.subheader("ğŸ”¥ ì„±ëŠ¥ íˆíŠ¸ë§µ")
                
                models = list(detailed_metrics.keys())
                metrics_names = ['Hit Rate', 'MRR', 'NDCG', 'Precision@K']
                performance_matrix = np.array([detailed_metrics[model] for model in models])
                
                fig_heatmap = px.imshow(
                    performance_matrix,
                    x=metrics_names,
                    y=[model.split('/')[-1] for model in models],
                    color_continuous_scale='RdYlGn',
                    title='ëª¨ë¸ Ã— ë©”íŠ¸ë¦­ ì„±ëŠ¥ íˆíŠ¸ë§µ',
                    text_auto='.3f'
                )
                fig_heatmap.update_layout(height=400)
                st.plotly_chart(fig_heatmap, use_container_width=True)
            
            # ì „ì²´ ê²°ê³¼ í…Œì´ë¸”
            st.subheader("ğŸ“‹ ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
            st.dataframe(pd.DataFrame(benchmark_results), use_container_width=True)

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´")
    
    if 'json_data' in st.session_state:
        items = st.session_state['json_data'].get('items', st.session_state['json_data'])
        st.success(f"âœ… ë°ì´í„° ë¡œë“œë¨: {len(items)}ê°œ ë¬¸ì„œ")
    else:
        st.info("ğŸ“ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”")
    
    # ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ ìƒíƒœ
    # LLM ìƒì„± íŒŒì¼ í™•ì¸
    try:
        llm_files = [f for f in os.listdir('.') if f.startswith('llm_benchmark_') and f.endswith('.json')]
        
        if llm_files:
            st.success(f"âœ… LLM ë²¤ì¹˜ë§ˆí¬: {len(llm_files)}ê°œ íŒŒì¼")
            latest_llm_file = max(llm_files, key=lambda x: os.path.getmtime(x))
            
            with open(latest_llm_file, 'r', encoding='utf-8') as f:
                llm_data = json.load(f)
            st.write(f"ğŸ“ ìµœì‹ : {latest_llm_file} ({len(llm_data)}ê°œ ì§ˆë¬¸)")
        else:
            st.info("ğŸ“ LLM ë²¤ì¹˜ë§ˆí¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”")
    except:
        st.info("ğŸ“ LLM ë²¤ì¹˜ë§ˆí¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”")
    
    # ì»¬ë ‰ì…˜ ìƒíƒœ
    try:
        chroma_client = get_chroma_client()
        collections = chroma_client.list_collections()
        if collections:
            st.success(f"âœ… ì»¬ë ‰ì…˜: {len(collections)}ê°œ êµ¬ì¶•ë¨")
            for col in collections:
                collection_obj = chroma_client.get_collection(col.name)
                st.write(f"- {col.name} ({collection_obj.count()}ê°œ)")
        else:
            st.info("ğŸ”§ ì»¬ë ‰ì…˜ì„ êµ¬ì¶•í•´ì£¼ì„¸ìš”")
    except:
        st.warning("âŒ ChromaDB ì—°ê²° ì‹¤íŒ¨")
    
    st.markdown("---")
    
    # ë¹ ë¥¸ ì•¡ì…˜ ë²„íŠ¼ë“¤
    st.subheader("âš¡ ë¹ ë¥¸ ì•¡ì…˜")
    
    if st.button("ğŸ”„ ì „ì²´ ì´ˆê¸°í™”", help="ëª¨ë“  ì„¸ì…˜ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.success("ì´ˆê¸°í™” ì™„ë£Œ!")
        st.rerun()
    
    if st.button("ğŸ“± ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬"):
        st.write("**GPU ìƒíƒœ:**", "âœ… ì‚¬ìš© ê°€ëŠ¥" if device == "cuda" else "âŒ ì‚¬ìš© ë¶ˆê°€")
        
        try:
            torch.cuda.empty_cache()
            if device == "cuda":
                memory_allocated = torch.cuda.memory_allocated() / 1024**3
                memory_reserved = torch.cuda.memory_reserved() / 1024**3
                st.write(f"**GPU ë©”ëª¨ë¦¬:** {memory_allocated:.1f}GB / {memory_reserved:.1f}GB")
        except:
            pass
        
        st.write(f"**ChromaDB ê²½ë¡œ:** {CHROMA_DB_PATH}")
    
    # ê³ ê¸‰ ì„¤ì •
    with st.expander("ğŸ”§ ê³ ê¸‰ ì„¤ì •"):
        debug_mode = st.checkbox("ë””ë²„ê·¸ ëª¨ë“œ")
        if debug_mode:
            st.write("**ì„¸ì…˜ ìƒíƒœ:**")
            for key in st.session_state.keys():
                if key == 'json_data':
                    st.write(f"- {key}: ë¡œë“œë¨")
                elif key == 'benchmark_results':
                    st.write(f"- {key}: {len(st.session_state[key])}ê°œ ê²°ê³¼")
                else:
                    st.write(f"- {key}: {type(st.session_state[key])}")
        
        auto_save = st.checkbox("ìë™ ì €ì¥", value=True)
        show_warnings = st.checkbox("ê²½ê³  í‘œì‹œ", value=True)
        
        if st.button("ğŸ§¹ ìºì‹œ ì •ë¦¬"):
            st.cache_resource.clear()
            st.cache_data.clear()
            st.success("ìºì‹œ ì •ë¦¬ ì™„ë£Œ!")

st.markdown("---")
st.markdown("**ğŸ’¡ ì‚¬ìš© íŒ:** íƒ­ì„ ìˆœì„œëŒ€ë¡œ ì§„í–‰í•˜ì„¸ìš”: ğŸ“ ë°ì´í„° ì¤€ë¹„ â†’ **ğŸ¤– LLM ì •í™•í•œ ë²¤ì¹˜ë§ˆí¬** â†’ ğŸ”§ ì»¬ë ‰ì…˜ êµ¬ì¶• â†’ ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ â†’ ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„")
st.markdown("**ğŸ¯ ê¶Œì¥:** LLM ë°©ì‹ìœ¼ë¡œ ì •í™•í•œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ìƒì„±í•˜ë©´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ ì¸¡ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
